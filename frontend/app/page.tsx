'use client';

import { useEffect, useRef, useState, useCallback, useMemo } from 'react';
import { Button } from '@/components/ui/button';
import { Card } from '@/components/ui/card';
import AudioCapture from '@/components/audio-capture';
import { Avatar, AvatarFallback, AvatarImage } from '@/components/ui/avatar';
import { User, Bot, Mic, MicOff, Power, PowerOff, X } from 'lucide-react';
import AudioCaptureMediaRecorder from '@/components/audio-capture-mediarecorder';
import { ToolOutput } from '@/components/tool-outputs/ToolOutput';
import type { ToolOutput as ToolOutputType } from '@/components/tool-outputs/types';
import { AudioPlaybackService } from '@/components/audio-playback';
import { motion, AnimatePresence } from 'framer-motion';

interface TextMessage {
  text: string;
  role: string;
}

// Add these message animation variants before the Home component
const messageVariants = {
  initial: { 
    opacity: 0, 
    y: 20,
    scale: 0.95
  },
  animate: { 
    opacity: 1, 
    y: 0,
    scale: 1,
    transition: {
      type: "spring",
      stiffness: 260,
      damping: 20
    }
  },
  exit: { 
    opacity: 0, 
    scale: 0.95,
    transition: { duration: 0.2 }
  }
};

const avatarVariants = {
  initial: { scale: 0 },
  animate: { 
    scale: 1,
    transition: {
      type: "spring",
      stiffness: 260,
      damping: 20,
      delay: 0.1
    }
  }
};

export default function Home() {
  const [status, setStatus] = useState('Disconnected');
  const [recording, setRecording] = useState(false);
  const [textOutputs, setTextOutputs] = useState<TextMessage[]>([]);
  const [toolUiOutput, setToolUiOutput] = useState<ToolOutputType | null>(null);
  const [currentTool, setCurrentTool] = useState<{name: string, content: string} | null>(null);
  const [waitingForTool, setWaitingForTool] = useState(false);
  const [wsKey, setWsKey] = useState(0);
  const [isThinking, setIsThinking] = useState(false);
  const [toolConfigs, setToolConfigs] = useState<any[]>([]);
  const [typingAudio] = useState(() => typeof Audio !== 'undefined' ? new Audio('/sounds/typing-short.mp3') : null);
  const typingSoundInterval = useRef<NodeJS.Timeout | null>(null);

  const wsRef = useRef<WebSocket | null>(null);
  const displayedMessages = useRef<Set<string>>(new Set());
  const displayedTextContentIds = useRef<Set<string>>(new Set());
  const contentIdToStage = useRef<Record<string, string>>({});
  const pendingTexts = useRef<Record<string, TextMessage>>({});
  const chatContainerRef = useRef<HTMLDivElement>(null);
  const playbackServiceRef = useRef<AudioPlaybackService | null>(null);

  // Debug-enhanced recording state setter
  const setRecordingWithDebug = useCallback((value: boolean) => {
    console.log(`[Recording State] Setting to ${value}. Triggered by:`, new Error().stack);
    setRecording(value);
  }, []);

  const connect = () => {
    if (wsRef.current) wsRef.current.close();
    wsRef.current = new WebSocket('ws://localhost:8000/ws');
    setStatus('Connecting...');
    setWsKey(k => k + 1);
    
    wsRef.current.onopen = () => setStatus('Connected');
    wsRef.current.onclose = () => {
      setStatus('Disconnected');
      setRecordingWithDebug(false);
    };
    wsRef.current.onerror = () => setStatus('Error');
    wsRef.current.onmessage = (event) => {
      if (typeof event.data === 'string') {
        try {
          const msg = JSON.parse(event.data);
          console.log('[WS MESSAGE]', msg);
          
          if (msg.event) handleEventMessage(msg.event);
        } catch (e) {
          console.error('Error parsing message:', e);
        }
      }
    };
  };

  // Memoize handlers to prevent unnecessary re-renders
  const handleAudioError = useCallback((error: Error) => {
    console.error('[Audio Error]', error);
    setRecordingWithDebug(false);
  }, [setRecordingWithDebug]);

  const addMessage = useCallback((message: TextMessage) => {
    const messageKey = `${message.role}:${message.text}`;
    if (!displayedMessages.current.has(messageKey)) {
      console.log('[ADDING MESSAGE]', message);
      displayedMessages.current.add(messageKey);
      setTextOutputs(prev => [...prev, message]);
    } else {
      console.log('[SKIPPING DUPLICATE]', message);
    }
  }, []);

  // Function to start looping the typing sound
  const startTypingSound = useCallback(() => {
    if (typingAudio) {
      // Set up the interval to play the sound repeatedly
      typingSoundInterval.current = setInterval(() => {
        typingAudio.currentTime = 0;
        typingAudio.play().catch(err => console.error('Error playing sound:', err));
      }, 1500); // Adjust interval as needed (current: 1.5 seconds)
    }
  }, [typingAudio]);

  // Function to stop the typing sound
  const stopTypingSound = useCallback(() => {
    if (typingSoundInterval.current) {
      clearInterval(typingSoundInterval.current);
      typingSoundInterval.current = null;
    }
    if (typingAudio) {
      typingAudio.pause();
      typingAudio.currentTime = 0;
    }
  }, [typingAudio]);

  // Clean up interval on unmount
  useEffect(() => {
    return () => {
      if (typingSoundInterval.current) {
        clearInterval(typingSoundInterval.current);
      }
    };
  }, []);

  const handleEventMessage = (event: any) => {
    console.log('[WS EVENT]', event);
    
    if (event.init) {
      // Store tool configurations
      setToolConfigs(event.init.toolConfigs);
      return;
    }
    
    if (event.contentStart && event.contentStart.type === 'TEXT') {
      const contentId = event.contentStart.contentId;
      let stage = 'FINAL';
      if (event.contentStart.additionalModelFields) {
        try {
          const fields = JSON.parse(event.contentStart.additionalModelFields);
          if (fields.generationStage) stage = fields.generationStage;
        } catch (e) {
          console.error('[TEXT STAGE ERROR]', e);
        }
      }
      if (contentId) {
        console.log('[TEXT STAGE]', contentId, stage);
        contentIdToStage.current[contentId] = stage;
      }
    } else if (event.contentStart && event.contentStart.type === 'TOOL') {
      console.log('[TOOL START]', event);
      setWaitingForTool(true);
    } else if (event.toolUse) {
      console.log('[TOOL USE]', event.toolUse);
      setWaitingForTool(true);
      const toolName = event.toolUse.toolName;
      
      // Find the tool configuration and get its short description
      const toolConfig = toolConfigs.find((t: any) => t.name === toolName);
      setCurrentTool({
        name: toolName || 'Unknown Tool',
        content: 'Processing...'
      });
    } else if (event.toolResult) {
      // Tool result received
      console.log('[TOOL RESULT FULL]', event.toolResult);
      try {
        const parsedContent = JSON.parse(event.toolResult.content);
        console.log('[TOOL RESULT PARSED]', parsedContent);
      } catch (e) {
        console.error('[TOOL RESULT PARSE ERROR]', e);
      }
      setWaitingForTool(false);
      setCurrentTool(prev => prev ? {
        ...prev,
        content: JSON.stringify(JSON.parse(event.toolResult.content), null, 2)
      } : null);
    } else if (event.toolUiOutput) {
      // Handle tool UI output
      console.log('[TOOL UI OUTPUT]', event.toolUiOutput);
      
      // Handle barge-in events
      if (event.toolUiOutput.type === 'barge_in') {
        console.log('[BARGE IN] Stopping audio playback');
        if (playbackServiceRef.current) {
          playbackServiceRef.current.stop();
        }
      }
      
      // Handle tool execution progress events
      if (event.toolUiOutput.type === 'tool_exec_progress') {
        const status = event.toolUiOutput.content.status;
        if (status === 'started') {
          startTypingSound();
        } else if (status === 'completed') {
          stopTypingSound();
        }
      }

      // Set the entire toolUiOutput object to preserve all fields (appName, props, etc)
      setToolUiOutput(event.toolUiOutput);
    } else if (event.contentEnd && event.contentEnd.type === 'TOOL') {
      // Tool usage ended
      console.log('[TOOL END]', event);
      setCurrentTool(null);
      setWaitingForTool(false);
    } else if (event.audioOutput) {
      // Stream audio chunk immediately
      const base64Audio = event.audioOutput.content;
      const audioBytes = base64ToArrayBuffer(base64Audio);
      if (playbackServiceRef.current) {
        playbackServiceRef.current.playPCM(audioBytes);
      }
      
      // If there's pending text for this contentId, show it
      const contentId = event.audioOutput.contentId || 'default';
      if (contentId && pendingTexts.current[contentId]) {
        const textMessage = pendingTexts.current[contentId];
        addMessage(textMessage);
        displayedTextContentIds.current.add(contentId);
        delete pendingTexts.current[contentId];
      }
    } else if (event.textOutput) {
      const text = event.textOutput.content;
      const role = event.textOutput.role || 'ASSISTANT';
      const contentId = event.textOutput.contentId;
      
      console.log('[TEXT EVENT]', {
        text,
        role,
        contentId,
        hasContentId: !!contentId,
        isDisplayed: displayedTextContentIds.current.has(contentId),
      });

      // Skip empty messages
      if (!text || text.trim() === '') {
        return;
      }

      // Show text immediately for USER and SYSTEM roles
      if (role === 'USER' || role === 'SYSTEM') {
        console.log('[SHOWING USER/SYSTEM TEXT]', { text, role });
        addMessage({ text, role });
        if (contentId) displayedTextContentIds.current.add(contentId);
      } else if (role === 'ASSISTANT') {
        // For assistant messages, check if we have corresponding audio
        const hasAudioOutput = event.audioOutput && event.audioOutput.contentId === contentId;
        
        // Show text immediately if no audio is expected
        if (!contentId || !hasAudioOutput) {
          console.log('[SHOWING ASSISTANT TEXT IMMEDIATELY]', { text, role });
          addMessage({ text, role });
          if (contentId) displayedTextContentIds.current.add(contentId);
        } else {
          // Buffer the text to sync with audio
          console.log('[BUFFERING ASSISTANT TEXT]', { text, contentId });
          pendingTexts.current[contentId] = { text, role };
        }
      }
    }
  };

  const disconnect = useCallback(() => {
    console.log('[Disconnect] Cleaning up connection');
    setRecordingWithDebug(false);
    setStatus('Disconnected');
    setTextOutputs([]);
    displayedMessages.current.clear();
    displayedTextContentIds.current.clear();
    contentIdToStage.current = {};
    pendingTexts.current = {};
    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
      setWsKey(k => k + 1);
    }
    if (playbackServiceRef.current) {
      playbackServiceRef.current.stop();
    }
  }, [setRecordingWithDebug]);

  const startRecording = useCallback(() => {
    console.log('[Start Recording]');
    setRecordingWithDebug(true);
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send('start_audio');
    }
  }, [setRecordingWithDebug]);

  const stopRecording = useCallback(() => {
    console.log('[Stop Recording]');
    setRecordingWithDebug(false);
    if (wsRef.current && wsRef.current.readyState === WebSocket.OPEN) {
      wsRef.current.send('stop_audio');
    }
  }, [setRecordingWithDebug]);

  // Memoize audio capture components
  const audioCaptureComponent = useMemo(() => (
    <AudioCapture
      key={wsKey}
      websocket={wsRef.current}
      isCapturing={recording}
      onError={handleAudioError}
      inline
      setIsThinking={setIsThinking}
    />
  ), [wsKey, recording, handleAudioError, setIsThinking]);

  const audioMediaRecorderComponent = useMemo(() => (
    <AudioCaptureMediaRecorder
      websocket={wsRef.current}
      isCapturing={recording}
      onError={handleAudioError}
      inline
      setIsThinking={setIsThinking}
      promptName={wsRef.current ? 'user_audio_prompt' : 'inactive_prompt'}
      contentName={wsRef.current ? `user_audio_${Date.now()}` : 'inactive_content'}
    />
  ), [wsKey, recording, handleAudioError, setIsThinking]);

  const base64LPCM = (base64String: string): string => {
    const byteCharacters = atob(base64String);
    const byteArrays = new Uint8Array(byteCharacters.length);
    
    for (let i = 0; i < byteCharacters.length; i++) {
      byteArrays[i] = byteCharacters.charCodeAt(i);
    }

    const sampleRate = 24000;
    const numChannels = 1;
    const bitsPerSample = 16;
    const byteRate = sampleRate * numChannels * (bitsPerSample / 8);
    const blockAlign = numChannels * (bitsPerSample / 8);
    const wavSize = byteArrays.length + 36;
    
    const wavHeader = new Uint8Array(44);
    const view = new DataView(wavHeader.buffer);

    let offset = 0;
    for (let i = 0; i < 4; i++) view.setUint8(offset++, "RIFF".charCodeAt(i));
    view.setUint32(offset, wavSize, true); offset += 4;
    for (let i = 0; i < 4; i++) view.setUint8(offset++, "WAVE".charCodeAt(i));
    for (let i = 0; i < 4; i++) view.setUint8(offset++, "fmt ".charCodeAt(i));
    view.setUint32(offset, 16, true); offset += 4;
    view.setUint16(offset, 1, true); offset += 2;
    view.setUint16(offset, numChannels, true); offset += 2;
    view.setUint32(offset, sampleRate, true); offset += 4;
    view.setUint32(offset, byteRate, true); offset += 4;
    view.setUint16(offset, blockAlign, true); offset += 2;
    view.setUint16(offset, bitsPerSample, true); offset += 2;
    for (let i = 0; i < 4; i++) view.setUint8(offset++, "data".charCodeAt(i));
    view.setUint32(offset, byteArrays.length, true); offset += 4;

    const wavBlob = new Blob([wavHeader, byteArrays], { type: "audio/wav" });
    return URL.createObjectURL(wavBlob);
  };

  // Helper function to convert base64 to ArrayBuffer
  const base64ToArrayBuffer = (base64: string): ArrayBuffer => {
    const binaryString = atob(base64);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    return bytes.buffer;
  };

  useEffect(() => {
    if (!playbackServiceRef.current) {
      playbackServiceRef.current = new AudioPlaybackService();
    }

    return () => {
      if (playbackServiceRef.current) {
        playbackServiceRef.current.stop();
      }
    };
  }, []);

  useEffect(() => {
    return () => {
      if (wsRef.current) {
        wsRef.current.close();
      }
    };
  }, []);

  // Add effect to log toolUiOutput state changes
  useEffect(() => {
    console.log('[TOOL UI OUTPUT STATE]', toolUiOutput);
  }, [toolUiOutput]);

  // Add clearToolOutput function
  const clearToolOutput = useCallback(() => {
    console.log('[CLEARING TOOL UI OUTPUT] by user');
    setToolUiOutput(null);
  }, []);

  // Add useEffect for auto-scrolling
  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, [textOutputs]); // Scroll when messages change

  const playTypingSound = useCallback(() => {
    if (typingAudio) {
      typingAudio.currentTime = 0; // Reset to start
      typingAudio.play().catch(err => console.error('Error playing sound:', err));
    }
  }, [typingAudio]);

  return (
    <main className="min-h-screen h-screen  flex flex-row items-stretch bg-gray-100">
      {/* Left pane for tool output */}
      <section className="w-full bg-white flex flex-col border-r border-gray-200 h-full overflow-hidden">
        <div className="p-2 border-0 border-gray-100 flex justify-between items-center shrink-0">
          {/* <Button
            onClick={playTypingSound}
            variant="ghost"
            className="h-8 px-4 hover:bg-gray-100"
          >
            Play Sound
          </Button> */}
          {toolUiOutput && (
            <Button
              onClick={clearToolOutput}
              variant="ghost"
              size="icon"
              className="h-8 w-8 rounded-full hover:bg-gray-100 ml-auto"
            >
              <X size={16} />
            </Button>
          )}
        </div>
        <div className="flex-1 overflow-y-auto p-6">
          {waitingForTool ? (
            <div className="h-full flex flex-col items-center justify-center text-gray-400 gap-3">
              <div className="animate-spin rounded-full h-8 w-8 border-2 border-gray-300 border-t-blue-600" />
              <p className="text-sm">Processing...</p>
            </div>
          ) : toolUiOutput ? (
            <ToolOutput output={toolUiOutput as ToolOutputType} websocket={wsRef.current} />
          ) : (
            <div className="h-full flex items-center justify-center text-gray-400 text-center">
              <div>
                <p className="mb-2">Display Canvas</p>
                <p className="text-sm">Nothing to show yet</p>
              </div>
            </div>
          )}
        </div>
      </section>

      {/* Right chat panel */}
      <aside className="w-full max-w-md flex flex-col h-screen justify-between bg-white border-r border-gray-200">
        <Card className="flex-1 shadow-xl border-0 p-8 space-y-6 bg-white h-full flex flex-col">
          <div className="flex items-center justify-between mb-2 border-b pb-4 border-gray-100">
            <div className="flex items-center gap-2">
              <Button
                onClick={status === 'Connected' ? disconnect : connect}
                className={`bg-black text-white hover:bg-neutral-800 border-0 shadow-none rounded-full p-0 w-10 h-10 flex items-center justify-center`}
                size="icon"
                aria-label={status === 'Connected' ? 'Disconnect' : 'Connect'}
              >
                {status === 'Connected' ? <PowerOff size={20} /> : <Power size={20} />}
              </Button>
              <Button
                onClick={recording ? stopRecording : startRecording}
                disabled={status !== 'Connected'}
                className={`bg-black text-white hover:bg-neutral-800 border-0 shadow-none rounded-full p-0 w-10 h-10 flex items-center justify-center ${status !== 'Connected' ? 'opacity-50' : ''}`}
                size="icon"
                aria-label={recording ? 'Stop Recording' : 'Start Recording'}
              >
                {recording ? <MicOff size={20} /> : <Mic size={20} />}
              </Button>
              <div className="ml-6 w-full">
                {audioCaptureComponent}
              </div>
            </div>
            <span className="ml-2 flex items-center">
              <span className="inline-block w-3 h-3 rounded-full bg-green-500 mr-2" title={status}></span>
            </span>
          </div>

          <div ref={chatContainerRef} className="h-96 overflow-y-auto space-y-4 bg-white p-0 rounded-lg border-0 border-gray-200 flex-1">
            <AnimatePresence>
              {textOutputs.map((msg, i) => (
                <motion.div
                  key={i}
                  layout
                  initial="initial"
                  animate="animate"
                  exit="exit"
                  variants={messageVariants}
                  className={`flex items-start gap-3 ${
                    msg.role === 'USER' ? 'justify-end' : 'justify-start'
                  }`}
                >
                  {msg.role === 'ASSISTANT' && (
                    <motion.div
                      variants={avatarVariants}
                      initial="initial"
                      animate="animate"
                    >
                      <Avatar className="bg-black text-white w-10 h-10 ring-2 ring-offset-2 ring-black/5">
                        <AvatarImage src="/nova-sonic-avatar.png" alt="Assistant" />
                        <AvatarFallback><Bot size={20} /></AvatarFallback>
                      </Avatar>
                    </motion.div>
                  )}
                  <motion.div
                    className={`p-3 rounded-2xl shadow-sm max-w-[70%] text-sm font-medium backdrop-blur-sm ${
                      msg.role === 'USER'
                        ? 'bg-black text-white rounded-br-none shadow-lg'
                        : msg.role === 'SYSTEM'
                        ? 'bg-gray-100/80 text-gray-500 text-sm italic'
                        : msg.role === 'ASSISTANT'
                        ? 'bg-gray-100/80 text-black rounded-bl-none shadow-md'
                        : 'bg-gray-100/80 text-black'
                    }`}
                    whileHover={{ scale: 1.02 }}
                    transition={{ type: "spring", stiffness: 400, damping: 17 }}
                  >
                    {msg.text}
                  </motion.div>
                  {msg.role === 'USER' && (
                    <motion.div
                      variants={avatarVariants}
                      initial="initial"
                      animate="animate"
                    >
                      <Avatar className="bg-black text-white w-10 h-10 ring-2 ring-offset-2 ring-black/5">
                        <AvatarFallback><User size={20} /></AvatarFallback>
                      </Avatar>
                    </motion.div>
                  )}
                </motion.div>
              ))}
            </AnimatePresence>
          </div>
        </Card>
      </aside>
    </main>
  );
}
